from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

processor = AutoProcessor.from_pretrained("ds4sd/SmolDocling-256M-preview")
model = AutoModelForVision2Seq.from_pretrained("ds4sd/SmolDocling-256M-preview")
model.to(device)

# === ä¿®å¤ boundaries è®¾å¤‡ä¸ä¸€è‡´çš„é—®é¢˜ ===
if hasattr(model, "boundaries"):
    model.boundaries = model.boundaries.to(device)
if hasattr(model, "vision_tower") and hasattr(model.vision_tower, "boundaries"):
    model.vision_tower.boundaries = model.vision_tower.boundaries.to(device)

# å¦‚æœ boundaries åœ¨æ›´æ·±å±‚ï¼š
for name, module in model.named_modules():
    if hasattr(module, "boundaries"):
        module.boundaries = module.boundaries.to(device)

image = Image.open("sample_page.png").convert("RGB")
prompt = "<image> Convert this page to docling."

raw_inputs = processor(text=prompt, images=image, return_tensors="pt", truncation=True)
allowed_keys = {"input_ids", "attention_mask", "pixel_values", "decoder_input_ids"}
inputs = {k: v.to(device) for k, v in raw_inputs.items() if k in allowed_keys}

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=1024)

text = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print("\nğŸ“„ Extracted Output:\n")
print(text)
